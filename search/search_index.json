{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"guides/odi-pipeline-tutorial/","text":"ODI Pipeline \u00b6 Quick Setup \u00b6 Using the information descried in the previous section, a quick setup can be performed by running the setup script with the appropriate values defined in the pipeline properties file. The steps are described below in detail. Installing Deploy master with ODI instance \u00b6 Extract the deployer tar package on the client machine. Log into the cluster where the pipeline should get deployed. Then proceed with the steps listed below: Move to the setup directory, cd pipeline/setup Create a copy from the properties template file. cp pipeline-properties.template pipeline-<cluster-name>.properties Replace the property variables with appropriate value as described in the table below: Property Description Example value odi.release ODI release version v2.0.4 also use as the version tag for the installer images. repo.public Name of the repo file for the GA entitlements. repo-public.yaml repo.staging Name of the repo file for the Staging area entitlements. repo-staging.yaml cluster.login.secret.name Cluster login secret used by the pipeline. <cluster>-login-secret imagepull.registry.key Installer container image name. us.icr.io/odi-stg/odi-installer deploy.namespace Namespace where pipeline will deploy. odi-deploy deploy.service.account Name of the service account used by the pipeline. odi-deployer deploy.file.storage Storage class used by pipeline and the OSDU component ibmc-file-gold deploy.block.storage Name of the repo file for the GA entitlements. ibmc-block-gold cluster.shortname Name of the repo file for the GA entitlements. cluster-shortname login.type Mode of authentication password or token. token pipeline.run.yaml Area that pipeline should use for the deploy. repo-public.yaml cpd.version Based Cloud Pak for Data version used by cpd-cli . 3.5.4 odi.install.namespace ODI install namespace. osdu hardware.arch Install hardware architecture. x86_64 external.cert.required Mode of authentication password or token. \"No\" Copy the repo yaml files into the setup directory. Run the setup script. ./setup.sh pipeline-<cluster-name>.properties all | tee pipeline-<cluster-name>-install.log This should install the pipeline under the specified project and create the pipeline run manifests under ../runs/<cluster-name> directory. To upgrade the version of ODI, cd ./runs/<cluster-name>; upgradeODI.sh Deploy setup for remote cluster \u00b6 Setting up remote cluster is a two step process. Login to the target cluster using admin credentials. Then proceed with the steps listed below: Create a copy from the properties template file to represent the target cluster. cp pipeline-properties.template pipeline-<cluster-name>.properties Move to the setup directory, cd pipeline/setup Run the setup script. ./setup.sh pipeline-<cluster-name>.properties setup-target | tee pipeline-<cluster-name>-target-install.log . This sets up the service account and image pull secrets under the odi deploy namespace. Note that the pipeline run manifests are generated under ../runs/<cluster-name> directory. Now, switch to the master deploy cluster and setup the remote cluster configuration. Login to the master cluster using admin credentials. Run the setup script. ./setup.sh pipeline-<cluster-name>.properties setup-remote | tee pipeline-<cluster-name>-remote-install.log . This sets up the the login secret and (optionally) repo configs under the odi deploy namespace. To upgrade the version of ODI, cd ./runs/<cluster-name>; upgradeODI.sh Using Deploy Automation components \u00b6 Installer image setup \u00b6 Follow these steps to create an ODI installer image. Clone the repo git@github.ibm.com:osdu/odi-deploy-automation.git and cd pipeline/ci-cd . This folder contains build-image-pipeline.properties template which needs to be updated with the relevant parameters as below. Property Description Example value installer.image Installer container image name. us.icr.io/odi-stg/odi-installer deployer.image Deployer container image name. us.icr.io/odi-stg/odi-deployer gitreadonlyaccestoken Git PAT token from github. ****** username IBM-SLB write secret username. ****** apikey IBM-SLB write secret apikey. ****** Login into the cluster where this pipeline will be created. Run the command create_config.sh which will create the namespace, service account, PVC, secret, tekton tasks and pipeline. Run the command build_installer_pipelinerun.sh <image-tag> with appropriate image tag with which the pipeline run needs to be created. After this, it will trigger this pipeline with the help of pipelinerun (build-installer-image-pipeline-run.yaml). The container image is then pushed to the image registry us.icr.io/odi-stg/ with the latest tag. Use the installer image for manual installation of the ODI as described in the Manual install section. Pipelines \u00b6 Manifests for Tekton constructs are under the /pipeline/manifests directory. These yaml files create the tasks and pipelines. Pipeline Description Supporting tasks login-pipeline Sets up login context for deploy and target clusters login-task uninstall-odi-pipeline Uninstalls ODI instance uninstall-odi install-odi-prereqs-pipeline Installs ODI prereqs install-odi-prereqs install-control-plane-pipeline Sets up CPD service accounts install-cpd-control-plane install-osdu-pipeline Installs adm lite and OSDU install-osdu run-post-install-pipeline Runs post install script run-post-install-script run-sanity-test-pipeline Runs sanity test run-sanity-test install-validate-pipeline Validates installation run-sanity-test, run-postman-collection upgrade-odi-pipeline Sets up login context for deploy and target clusters login-task, uninstall-odi, install-odi-prereqs, install-cpd-control-plane, install-osdu, run-post-install-script Pipeline runs \u00b6 Pipeline runs manage the execution of pipelines across various clusters. Pipeline runs are generated per cluster and kept under the pipeline/runs directory. Commands \u00b6 installOpenShiftPipeline.sh Installs OpenShift pipelines operator installODIDeployPipeline.sh Installs ODI pipeline deploy artifacts. createServiceAccount.sh Sets up the remote cluster for ODI deploy. Creates the namespace and the service account. uninstallPipeline.sh Uninstalls deploy artifacts, deletes the pipeline namespace and uninstalls the pipeline operator. Building the deployer image \u00b6 Follow these steps to create an ODI deployer image. Clone the repo git@github.ibm.com:osdu/odi-deploy-automation.git and cd pipeline/ci-cd . This folder contains build-image-pipeline.properties template which needs to be updated with the relevant parameters. Login into the cluster where this pipeline will be created. Run the command create_config.sh which will create the namespace, service account, PVC, secret, tekton tasks and pipeline. Run the command build_deployer_pipelinerun.sh <image-tag> with appropriate image tag with which the pipeline run needs to be created. After this, it will trigger this pipeline with the help of pipelinerun (build-deployer-image-pipeline-run.yaml). The container image is then pushed to the image registry us.icr.io/odi-stg/ with the latest tag. Running the pipelines using the Tekton CLI. \u00b6 More information here . Uninstalling pipeline \u00b6 Uninstalling pipeline deletes the odi-deploy namespace and uninstalls the OpenShift pipelines operator. Run the commands listed below: cd setup; uninstallPipeline.sh Manual installation \u00b6 Un-tar the deployer package or get the docker image to run a manual installation. Login to the registry us.icr.io using the ready key and then run the following command to setup the install client using the docker container. docker run -it us.icr.io/odi-stg/odi-installer:<version> bash Install ODI using the script cd installer; ./installODI.sh Cluster Provisioning \u00b6 This package provides a setup of Terraform scripts to provision an OpenShift cluster in IBM Cloud with the specifications required to deploy an ODI instance. Steps to follow: Setup the variables. Run the command terraform plan Review the infrastructure items that will get created. Run terraform apply Environments \u00b6 Follow this link to view all the environments managed by the deploy pipeline. Getting Pipeline backup logs on local \u00b6 Go to the setup directory. Run the script ./copy-backup-local.sh PIPELINE_RUN_NAME This will create a tar file i.e backup-log-files.tar which contains logs for a particular Pipeline-run. Using BuildConfig to build installer/deployer images \u00b6 Setup buildconfigs for Dev or Integration clusters \u00b6 Run the script setupBuildConfigforDev.sh passing the respective values i.e.., BRANCH_NAME of the repo OR TAG for the image, base64 encoded username and password for the git source clone This will create the namespace, ImageStreams, secrets and buildConfigs for building installer and deployer images. Once the buildConfigs for installer and deployer images are created, OpenShift will create a new build for the respective buildConfigs and pushed to the internal registry. Now for auto-triggering the builds for consecutive git pushes, please follow the below steps: After creating a BuildConfig from a GitHub repository, run: oc describe bc/ -n ci-cd | awk '/webhooks/ {print $2}' This generates a generic webhook GitHub URL Also run the below command for retrieving the webhook secret name oc get bc/ -o=jsonpath='{.spec.triggers..github.secret}' -n ci-cd Either copy the entire webhook URL with the secret name in it by replacing the in webhook generic URL from the above commands OR you can Copy URL with secret in the details of the buildConfig for installer or deployer buildConfig in the OpenShift cluster UI Adding the OpenShift created webhook URL to GitHub Now this webhook URL with the webhook secret name in it must be added to the hooks in GitHub setting on which repository the trigger for webhook is configured to. For creating a new hook on Github, go to the settings in the repository, go to the hooks, click on add a webhook, paste the webhook URL in the payload URL which can be retrieved from the above steps, change the content type to application/json, add the webhook secret created from the above steps and then click on add webhook. Unique webhooks for installer buildConfig and deployer buildConfig must be created so that whenever a git push happens to the given branch in buildConfig, unique payloads will be sent to both the installer and deployer build configs.","title":"ODI Pipeline"},{"location":"guides/odi-pipeline-tutorial/#odi-pipeline","text":"","title":"ODI Pipeline"},{"location":"guides/odi-pipeline-tutorial/#quick-setup","text":"Using the information descried in the previous section, a quick setup can be performed by running the setup script with the appropriate values defined in the pipeline properties file. The steps are described below in detail.","title":"Quick Setup"},{"location":"guides/odi-pipeline-tutorial/#installing-deploy-master-with-odi-instance","text":"Extract the deployer tar package on the client machine. Log into the cluster where the pipeline should get deployed. Then proceed with the steps listed below: Move to the setup directory, cd pipeline/setup Create a copy from the properties template file. cp pipeline-properties.template pipeline-<cluster-name>.properties Replace the property variables with appropriate value as described in the table below: Property Description Example value odi.release ODI release version v2.0.4 also use as the version tag for the installer images. repo.public Name of the repo file for the GA entitlements. repo-public.yaml repo.staging Name of the repo file for the Staging area entitlements. repo-staging.yaml cluster.login.secret.name Cluster login secret used by the pipeline. <cluster>-login-secret imagepull.registry.key Installer container image name. us.icr.io/odi-stg/odi-installer deploy.namespace Namespace where pipeline will deploy. odi-deploy deploy.service.account Name of the service account used by the pipeline. odi-deployer deploy.file.storage Storage class used by pipeline and the OSDU component ibmc-file-gold deploy.block.storage Name of the repo file for the GA entitlements. ibmc-block-gold cluster.shortname Name of the repo file for the GA entitlements. cluster-shortname login.type Mode of authentication password or token. token pipeline.run.yaml Area that pipeline should use for the deploy. repo-public.yaml cpd.version Based Cloud Pak for Data version used by cpd-cli . 3.5.4 odi.install.namespace ODI install namespace. osdu hardware.arch Install hardware architecture. x86_64 external.cert.required Mode of authentication password or token. \"No\" Copy the repo yaml files into the setup directory. Run the setup script. ./setup.sh pipeline-<cluster-name>.properties all | tee pipeline-<cluster-name>-install.log This should install the pipeline under the specified project and create the pipeline run manifests under ../runs/<cluster-name> directory. To upgrade the version of ODI, cd ./runs/<cluster-name>; upgradeODI.sh","title":"Installing Deploy master with ODI instance"},{"location":"guides/odi-pipeline-tutorial/#deploy-setup-for-remote-cluster","text":"Setting up remote cluster is a two step process. Login to the target cluster using admin credentials. Then proceed with the steps listed below: Create a copy from the properties template file to represent the target cluster. cp pipeline-properties.template pipeline-<cluster-name>.properties Move to the setup directory, cd pipeline/setup Run the setup script. ./setup.sh pipeline-<cluster-name>.properties setup-target | tee pipeline-<cluster-name>-target-install.log . This sets up the service account and image pull secrets under the odi deploy namespace. Note that the pipeline run manifests are generated under ../runs/<cluster-name> directory. Now, switch to the master deploy cluster and setup the remote cluster configuration. Login to the master cluster using admin credentials. Run the setup script. ./setup.sh pipeline-<cluster-name>.properties setup-remote | tee pipeline-<cluster-name>-remote-install.log . This sets up the the login secret and (optionally) repo configs under the odi deploy namespace. To upgrade the version of ODI, cd ./runs/<cluster-name>; upgradeODI.sh","title":"Deploy setup for remote cluster"},{"location":"guides/odi-pipeline-tutorial/#using-deploy-automation-components","text":"","title":"Using Deploy Automation components"},{"location":"guides/odi-pipeline-tutorial/#installer-image-setup","text":"Follow these steps to create an ODI installer image. Clone the repo git@github.ibm.com:osdu/odi-deploy-automation.git and cd pipeline/ci-cd . This folder contains build-image-pipeline.properties template which needs to be updated with the relevant parameters as below. Property Description Example value installer.image Installer container image name. us.icr.io/odi-stg/odi-installer deployer.image Deployer container image name. us.icr.io/odi-stg/odi-deployer gitreadonlyaccestoken Git PAT token from github. ****** username IBM-SLB write secret username. ****** apikey IBM-SLB write secret apikey. ****** Login into the cluster where this pipeline will be created. Run the command create_config.sh which will create the namespace, service account, PVC, secret, tekton tasks and pipeline. Run the command build_installer_pipelinerun.sh <image-tag> with appropriate image tag with which the pipeline run needs to be created. After this, it will trigger this pipeline with the help of pipelinerun (build-installer-image-pipeline-run.yaml). The container image is then pushed to the image registry us.icr.io/odi-stg/ with the latest tag. Use the installer image for manual installation of the ODI as described in the Manual install section.","title":"Installer image setup"},{"location":"guides/odi-pipeline-tutorial/#pipelines","text":"Manifests for Tekton constructs are under the /pipeline/manifests directory. These yaml files create the tasks and pipelines. Pipeline Description Supporting tasks login-pipeline Sets up login context for deploy and target clusters login-task uninstall-odi-pipeline Uninstalls ODI instance uninstall-odi install-odi-prereqs-pipeline Installs ODI prereqs install-odi-prereqs install-control-plane-pipeline Sets up CPD service accounts install-cpd-control-plane install-osdu-pipeline Installs adm lite and OSDU install-osdu run-post-install-pipeline Runs post install script run-post-install-script run-sanity-test-pipeline Runs sanity test run-sanity-test install-validate-pipeline Validates installation run-sanity-test, run-postman-collection upgrade-odi-pipeline Sets up login context for deploy and target clusters login-task, uninstall-odi, install-odi-prereqs, install-cpd-control-plane, install-osdu, run-post-install-script","title":"Pipelines"},{"location":"guides/odi-pipeline-tutorial/#manual-installation","text":"Un-tar the deployer package or get the docker image to run a manual installation. Login to the registry us.icr.io using the ready key and then run the following command to setup the install client using the docker container. docker run -it us.icr.io/odi-stg/odi-installer:<version> bash Install ODI using the script cd installer; ./installODI.sh","title":"Manual installation"},{"location":"guides/odi-pipeline-tutorial/#cluster-provisioning","text":"This package provides a setup of Terraform scripts to provision an OpenShift cluster in IBM Cloud with the specifications required to deploy an ODI instance. Steps to follow: Setup the variables. Run the command terraform plan Review the infrastructure items that will get created. Run terraform apply","title":"Cluster Provisioning"},{"location":"guides/odi-pipeline-tutorial/#environments","text":"Follow this link to view all the environments managed by the deploy pipeline.","title":"Environments"},{"location":"guides/odi-pipeline-tutorial/#getting-pipeline-backup-logs-on-local","text":"Go to the setup directory. Run the script ./copy-backup-local.sh PIPELINE_RUN_NAME This will create a tar file i.e backup-log-files.tar which contains logs for a particular Pipeline-run.","title":"Getting Pipeline backup logs on local"},{"location":"guides/odi-pipeline-tutorial/#using-buildconfig-to-build-installerdeployer-images","text":"","title":"Using BuildConfig to build installer/deployer images"},{"location":"guides/odi-pipeline-tutorial/#setup-buildconfigs-for-dev-or-integration-clusters","text":"Run the script setupBuildConfigforDev.sh passing the respective values i.e.., BRANCH_NAME of the repo OR TAG for the image, base64 encoded username and password for the git source clone This will create the namespace, ImageStreams, secrets and buildConfigs for building installer and deployer images. Once the buildConfigs for installer and deployer images are created, OpenShift will create a new build for the respective buildConfigs and pushed to the internal registry. Now for auto-triggering the builds for consecutive git pushes, please follow the below steps: After creating a BuildConfig from a GitHub repository, run: oc describe bc/ -n ci-cd | awk '/webhooks/ {print $2}' This generates a generic webhook GitHub URL Also run the below command for retrieving the webhook secret name oc get bc/ -o=jsonpath='{.spec.triggers..github.secret}' -n ci-cd Either copy the entire webhook URL with the secret name in it by replacing the in webhook generic URL from the above commands OR you can Copy URL with secret in the details of the buildConfig for installer or deployer buildConfig in the OpenShift cluster UI Adding the OpenShift created webhook URL to GitHub Now this webhook URL with the webhook secret name in it must be added to the hooks in GitHub setting on which repository the trigger for webhook is configured to. For creating a new hook on Github, go to the settings in the repository, go to the hooks, click on add a webhook, paste the webhook URL in the payload URL which can be retrieved from the above steps, change the content type to application/json, add the webhook secret created from the above steps and then click on add webhook. Unique webhooks for installer buildConfig and deployer buildConfig must be created so that whenever a git push happens to the given branch in buildConfig, unique payloads will be sent to both the installer and deployer build configs.","title":"Setup buildconfigs for Dev or Integration clusters"},{"location":"guides/sno-kvm-vsi-vpc-ibmcloud/","text":"Install single node OpenShift on Linux-KVM Virtual Server for VPC on IBM Cloud \u00b6 This guide uses a collection of Ansible playbooks that provisions a single node OpenShift cluster on Linux KVM on a Virtual Server for VPC on IBM Cloud using Red Hat Assisted Service (Software as a Service). The playbooks are bundled into the following three modules: ic.vpc creates the VPC Infrastructure on IBM Cloud, including VPC, Subnet, Floating IP, Access Control List (ACL), Security Group, Floating IP, ic.vsi.kvm creates a Virtual Server instance (VSI) for VPC on IBM Cloud and install Rocky Linux with Linux-KVM hypervisor, and required SSH keys, ic.vsi.kvm.sno creates the required Virtual Machines (VMs) and install single node OpenShift (SNO) on KVM-Linux, including KVM guest users, for DNS and DHCP services, HAProxy. For the complete installation guide go to https://github.com/IBM/sno-on-ibm-cloud-vpc-ansible .","title":"Install SNO on KVM"},{"location":"guides/sno-kvm-vsi-vpc-ibmcloud/#install-single-node-openshift-on-linux-kvm-virtual-server-for-vpc-on-ibm-cloud","text":"This guide uses a collection of Ansible playbooks that provisions a single node OpenShift cluster on Linux KVM on a Virtual Server for VPC on IBM Cloud using Red Hat Assisted Service (Software as a Service). The playbooks are bundled into the following three modules: ic.vpc creates the VPC Infrastructure on IBM Cloud, including VPC, Subnet, Floating IP, Access Control List (ACL), Security Group, Floating IP, ic.vsi.kvm creates a Virtual Server instance (VSI) for VPC on IBM Cloud and install Rocky Linux with Linux-KVM hypervisor, and required SSH keys, ic.vsi.kvm.sno creates the required Virtual Machines (VMs) and install single node OpenShift (SNO) on KVM-Linux, including KVM guest users, for DNS and DHCP services, HAProxy. For the complete installation guide go to https://github.com/IBM/sno-on-ibm-cloud-vpc-ansible .","title":"Install single node OpenShift on Linux-KVM Virtual Server for VPC on IBM Cloud"},{"location":"overview/overview/","text":"Industry Toolkit \u00b6 Overview \u00b6 The Industry Toolkit is a collection of assets that enable system development in Industry 4.0. Assets include architectures, use cases, Ansible playbooks, Tekton resources and guides in the Industry 4.0 space created by the IBM Build Labs team. The assets in this toolkit can be used as tools to learn or to help with the provisioning, configuration and installation of various software resources and systems, inspired by the work we did with IBM clients in Industry 4.0. Use Cases \u00b6 Edge Data Gateway - devices on the edge can be small appliances that function as a data gateway. A edge data gatewaty may run on a single node device as small as 2 cores with 4Gb RAM (2x4). To still profit from container and container orchestration like Kubernetes, you need an optimized edition of Kubernetes or OpenShift that can run on a single node cluster, like single node OpenShift (SNO) or MicroShift . Guides \u00b6 ODI DevOps Pipeline - The ODI DevOps Pipeline uses a collection of Tekton tasks intended for DevOps admins looking to setup and run an automated install of ODI onto OpenShift clusters. IBM Open Data for Industries (ODI) runs on Red Hat OpenShift and is built on the Open Subsurface Data Universe (OSDU) data foundation for the oil, gas and energy industry. ODI integrates fully with an IBM Cloud Pak for Data (CP4D) and can expedite data handling across on-prem, public cloud, multicloud or at the edge.","title":"What is the Industry Toolkit"},{"location":"overview/overview/#industry-toolkit","text":"","title":"Industry Toolkit"},{"location":"overview/overview/#overview","text":"The Industry Toolkit is a collection of assets that enable system development in Industry 4.0. Assets include architectures, use cases, Ansible playbooks, Tekton resources and guides in the Industry 4.0 space created by the IBM Build Labs team. The assets in this toolkit can be used as tools to learn or to help with the provisioning, configuration and installation of various software resources and systems, inspired by the work we did with IBM clients in Industry 4.0.","title":"Overview"},{"location":"overview/overview/#use-cases","text":"Edge Data Gateway - devices on the edge can be small appliances that function as a data gateway. A edge data gatewaty may run on a single node device as small as 2 cores with 4Gb RAM (2x4). To still profit from container and container orchestration like Kubernetes, you need an optimized edition of Kubernetes or OpenShift that can run on a single node cluster, like single node OpenShift (SNO) or MicroShift .","title":"Use Cases"},{"location":"overview/overview/#guides","text":"ODI DevOps Pipeline - The ODI DevOps Pipeline uses a collection of Tekton tasks intended for DevOps admins looking to setup and run an automated install of ODI onto OpenShift clusters. IBM Open Data for Industries (ODI) runs on Red Hat OpenShift and is built on the Open Subsurface Data Universe (OSDU) data foundation for the oil, gas and energy industry. ODI integrates fully with an IBM Cloud Pak for Data (CP4D) and can expedite data handling across on-prem, public cloud, multicloud or at the edge.","title":"Guides"},{"location":"use-cases/edge-data-gateway/","text":"Use Case: Edge Data Gateway \u00b6 Overview \u00b6 Devices on the edge can be small appliances that function as a data gateway. A edge data gatewaty may run on a single node device as small as 2 cores with 4Gb RAM (2x4). To still profit from container and container orchestration like Kubernetes, you need an optimized edition of Kubernetes or OpenShift that can run on a single node cluster, like single node OpenShift (SNO) or MicroShift. Ansible \u00b6 This use case uses a collection of Ansible playbooks that provisions a Single Node OpenShift cluster on Linux KVM on IBM Cloud using Red Hat Assisted Service. Installation Guide \u00b6 See Install single node OpenShift on Linux-KVM Virtual Server for VPC on IBM Cloud .","title":"Edge Data Gateway"},{"location":"use-cases/edge-data-gateway/#use-case-edge-data-gateway","text":"","title":"Use Case: Edge Data Gateway"},{"location":"use-cases/edge-data-gateway/#overview","text":"Devices on the edge can be small appliances that function as a data gateway. A edge data gatewaty may run on a single node device as small as 2 cores with 4Gb RAM (2x4). To still profit from container and container orchestration like Kubernetes, you need an optimized edition of Kubernetes or OpenShift that can run on a single node cluster, like single node OpenShift (SNO) or MicroShift.","title":"Overview"},{"location":"use-cases/edge-data-gateway/#ansible","text":"This use case uses a collection of Ansible playbooks that provisions a Single Node OpenShift cluster on Linux KVM on IBM Cloud using Red Hat Assisted Service.","title":"Ansible"},{"location":"use-cases/edge-data-gateway/#installation-guide","text":"See Install single node OpenShift on Linux-KVM Virtual Server for VPC on IBM Cloud .","title":"Installation Guide"}]}